{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DamrT3AqUxKd"
      },
      "source": [
        "# Design Thinking for Document Data Exploration from the Prototype code\n",
        "**Group 6-Members**\n",
        "* 110C51505 張仁程\n",
        "* 110C51537 王星文\n",
        "* 110C51522 謝安\n",
        "* 110C51526\t蔡侑芯\n",
        "* 110C51533\t陳毓庭\n",
        "* 110C51520\t楊文均"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#如果在Windows或Linux環境上請跑此段。\n",
        "#請先pip install gdown.\n",
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=10G2D9fm3UL92W2UuKdrw4bzJkDu0OflJ'\n",
        "output = '/Code/NTUTDS/patent_documents/patentdoc.tar.gz'\n",
        "#請使用自己的path\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#如果在Windows或Linux環境上請跑此段。\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def tar_extract(file_path):\n",
        "    tar = tarfile.open(file_path, 'r:gz')\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    file_path = '/Code/NTUTDS/patent_documents/patentdoc.tar.gz'\n",
        "    tar_extract(file_path)\n",
        "#請使用自己的path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#老師原始程式碼，在Colab上請跑此段。\n",
        "#!wget \"https://drive.google.com/uc?export=download&id=10G2D9fm3UL92W2UuKdrw4bzJkDu0OflJ\" -O patentdoc.tar.gz\n",
        "#!tar xvfz patentdoc.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#資料抓取與處理-在Pandas做merge\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "path = \"patent_documents\"\n",
        "patent_csv = {}\n",
        "for filename in os.listdir(path):\n",
        "  if filename.endswith(\".txt\"):\n",
        "    with open(os.path.join(path, filename), 'r') as f:\n",
        "      soup = BeautifulSoup(f.read(), 'xml')\n",
        "      patent_number = soup.find(\"publication-reference\").findChild(\"doc-number\").getText()\n",
        "      pub_date = soup.find(\"publication-reference\").findChild(\"date\").getText()\n",
        "      country = soup.find(\"publication-reference\").findChild(\"country\").getText()\n",
        "      app_ref = soup.find(\"application-reference\") #這個不要輸出\n",
        "      app_type = app_ref['appl-type']\n",
        "      app_date = app_ref.findChild(\"date\").getText()\n",
        "      patent_title = soup.find(\"invention-title\").getText()\n",
        "      patent_abstract = soup.find(\"abstract\").getText()\n",
        "      #Agents = soup.find(\"orgname\").getText() #如果文件沒有orgname這邊會return error\n",
        "      us_applicants = soup.find(\"us-applicants\") #這個不要輸出\n",
        "      app_firstname = us_applicants.findChild(\"first-name\").contents\n",
        "      app_lastname = us_applicants.findChild(\"last-name\").contents\n",
        "      applicant_name = ' '.join([a+b for a,b in zip(app_firstname,app_lastname)]) #記得之後要解決a跟b中間沒空格的問題\n",
        "      inventors = soup.find(\"inventors\") #這個不要輸出\n",
        "      inv_firstname = inventors.findChild(\"first-name\").contents\n",
        "      inv_lastname = inventors.findChild(\"last-name\").contents\n",
        "      inv_name = ' '.join([a+b for a,b in zip(inv_firstname,inv_lastname)]) #記得之後要解決a跟b中間沒空格的問題\n",
        "      ipcr_ref = soup.find(\"classification-ipcr\") #這個不要輸出\n",
        "      ipcr_section = ipcr_ref.findChild(\"section\").contents\n",
        "      ipcr_class = ipcr_ref.findChild(\"class\").contents\n",
        "      for class_num in ipcr_class:\n",
        "            if len(str(class_num))==1:\n",
        "                  class_num = str(\"0\") + class_num\n",
        "      ipcr_subclass = ipcr_ref.findChild(\"subclass\").contents\n",
        "      ipc_list = ' '.join([d+e+f for d,e,f in zip(ipcr_section,ipcr_class,ipcr_subclass)])\n",
        "      patent_csv = {\"Patent Number\":patent_number,\"Publication Date\":pub_date,\"Application Date\":app_date,\"Country\":country,\"Application Type\":app_type,\"Title\":patent_title.replace('\\n',' '),\"Abstract\":patent_abstract.replace('\\n',' '),\"Applicant\":applicant_name,\"Inventor\":inv_name,\"IPCs\":ipc_list}\n",
        "      \n",
        "      dfs = pd.DataFrame.from_dict(patent_csv,orient='index').T\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#資料抓取與處理2-在dict做merge\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "path = \"patent_documents\"\n",
        "patent_csv = {}\n",
        "for filename in os.listdir(path):\n",
        "  if filename.endswith(\".txt\"):\n",
        "    with open(os.path.join(path, filename), 'r') as f:\n",
        "      soup = BeautifulSoup(f.read(), 'xml')\n",
        "      patent_number = soup.find(\"publication-reference\").findChild(\"doc-number\").getText()\n",
        "      pub_date = soup.find(\"publication-reference\").findChild(\"date\").getText()\n",
        "      \n",
        "      \n",
        "patent_csv = {\"Patent Number\":patent_number,\"Publication Date\":pub_date}\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Deal only claim texts\n",
        "import pprint\n",
        "path = \"patent_documents\"\n",
        "patent_docs_txt = {}\n",
        "for filename in os.listdir(path):\n",
        "  if filename.endswith(\".txt\"):\n",
        "    with open(os.path.join(path, filename), 'r') as f:\n",
        "      soup = BeautifulSoup(f.read(), 'xml')\n",
        "      tag = soup.find_all(\"claims\")\n",
        "      patent_docs_txt[filename] = tag[0].text.lower().replace('\\n',' ')\n",
        "\n",
        "pprint.pprint(patent_docs_txt, depth=1, width=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bljWYPMPpTSo",
        "outputId": "c391ecf0-20c2-4b69-cd25-5d6c09d5232a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "print(nltk.__version__)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def Patent_KW50(patent_text):\n",
        "  tokens = nltk.word_tokenize(patent_text)\n",
        "  fdist = nltk.FreqDist(tokens)\n",
        "  common50 = fdist.most_common(50)\n",
        "  common50_new1 = [(k,v) for k,v in common50 if not re.fullmatch(r\"^.$\", k)]\n",
        "  common50_new2 = [(k,v) for k,v in common50_new1 if re.fullmatch(r\"^[a-z]+$\", k, flags=re.IGNORECASE)]\n",
        "  stopword = list(nltk_stopwords)[:100]\n",
        "  common50_new3 = dict([(k,v) for k,v in common50_new2 if not k in stopword])\n",
        "  return common50_new3\n",
        "\n",
        "patent_docs_KW = {}\n",
        "for doc_name, txt in patent_docs_txt.items():\n",
        "  patent_docs_KW[doc_name] = Patent_KW50(txt)\n",
        "\n",
        "pprint.pprint(patent_docs_KW, width=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nOulhJ7g5mJS",
        "outputId": "1d5cfc00-cf48-4b85-e97f-88841993fdf9"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import islice\n",
        "\n",
        "plt.clf()\n",
        "\n",
        "for doc_name, KW_stat in patent_docs_KW.items():\n",
        "  common20 = dict(islice(KW_stat.items(), 20))\n",
        "  print(common20)\n",
        "  x = np.arange(20)\n",
        "  y = common20.values()\n",
        "  plt.title( doc_name, fontsize=8, color='g')\n",
        "  plt.xticks(x, common20.keys(),  rotation='vertical')\n",
        "  plt.bar(x, y, color='y')\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Patent_parser.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
