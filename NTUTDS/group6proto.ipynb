{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#如果在Windows或Linux環境上請跑此段。\n",
        "#請先pip install gdown.\n",
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=10G2D9fm3UL92W2UuKdrw4bzJkDu0OflJ'\n",
        "output = '/Code/NTUTDS/patent_documents/patentdoc.tar.gz'\n",
        "#請使用自己的path\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#如果在Windows或Linux環境上請跑此段。\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def tar_extract(file_path):\n",
        "    tar = tarfile.open(file_path, 'r:gz')\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    file_path = '/Code/NTUTDS/patent_documents/patentdoc.tar.gz'\n",
        "    tar_extract(file_path)\n",
        "#請使用自己的path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#老師原始程式碼，在Colab上請跑此段。\n",
        "#!wget \"https://drive.google.com/uc?export=download&id=10G2D9fm3UL92W2UuKdrw4bzJkDu0OflJ\" -O patentdoc.tar.gz\n",
        "#!tar xvfz patentdoc.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#資料抓取與處理\n",
        "import pprint\n",
        "from bs4 import BeautifulSoup\n",
        "path = \"patent_documents\"\n",
        "patent_csv = {}\n",
        "patent_number = []\n",
        "pub_date = [] \n",
        "country = []\n",
        "app_type = []\n",
        "app_date = []\n",
        "patent_title = []\n",
        "patent_abstract = []\n",
        "applicant_name = []\n",
        "inv_name = []\n",
        "ipc_list = []\n",
        "for filename in os.listdir(path):\n",
        "  if filename.endswith(\".txt\"):\n",
        "    with open(os.path.join(path, filename), 'r') as f:\n",
        "      soup = BeautifulSoup(f.read(), 'xml')\n",
        "      panum = soup.find(\"publication-reference\").findChild(\"doc-number\").contents # contents抓出來的東西資料類型是list...\n",
        "      for pn in panum:\n",
        "            patent_number.append(pn)\n",
        "      pdate = soup.find(\"publication-reference\").findChild(\"date\").contents\n",
        "      for pd in pdate:\n",
        "             pub_date.append(pd)\n",
        "      ctry = soup.find(\"publication-reference\").findChild(\"country\").contents\n",
        "      for ct in ctry:\n",
        "            country.append(ct)\n",
        "      app_ref = soup.find(\"application-reference\") #這個不要輸出\n",
        "      atype = [app_ref['appl-type']]\n",
        "      for at in atype:\n",
        "            app_type.append(at)\n",
        "      adate = app_ref.findChild(\"date\").contents\n",
        "      for ad in adate:\n",
        "            app_date.append(ad)\n",
        "      ptitle = [soup.find(\"invention-title\").getText()] #要想辦法再把/n去掉\n",
        "      for pt in ptitle:\n",
        "            patent_title.append(pt)\n",
        "      pabs = [soup.find(\"abstract\").getText()] #要想辦法再把/n去掉\n",
        "      for ab in pabs:\n",
        "            patent_abstract.append(ab)\n",
        "      #Agents = soup.find(\"orgname\").getText() #如果文件沒有orgname這邊會return error\n",
        "      us_applicants = soup.find(\"us-applicants\") #這個不要輸出\n",
        "      app_firstname = us_applicants.findChild(\"first-name\").contents\n",
        "      app_lastname = us_applicants.findChild(\"last-name\").contents\n",
        "      aname = [' '.join([a+b for a,b in zip(app_firstname,app_lastname)])] #記得之後要解決a跟b中間沒空格的問題 \n",
        "      for an in aname:\n",
        "               applicant_name.append(an)\n",
        "      inventors = soup.find(\"inventors\") #這個不要輸出\n",
        "      inv_firstname = inventors.findChild(\"first-name\").contents\n",
        "      inv_lastname = inventors.findChild(\"last-name\").contents\n",
        "      iname = [' '.join([c+d for c,d in zip(inv_firstname,inv_lastname)])] #記得之後要解決a跟b中間沒空格的問題\n",
        "      for ina in iname:\n",
        "            inv_name.append(ina)\n",
        "      ipcr_ref = soup.find(\"classification-ipcr\") #這個不要輸出\n",
        "      ipcr_section = ipcr_ref.findChild(\"section\").contents\n",
        "      ipcr_class = ipcr_ref.findChild(\"class\").contents\n",
        "      for class_num in ipcr_class:\n",
        "            if len(str(class_num))==1:\n",
        "                  class_num = str(\"0\") + class_num\n",
        "      ipcr_subclass = ipcr_ref.findChild(\"subclass\").contents\n",
        "      ipcs = [' '.join([d+e+f for d,e,f in zip(ipcr_section,ipcr_class,ipcr_subclass)])]\n",
        "      for ip in ipcs:\n",
        "            ipc_list.append(ip)\n",
        "\n",
        "patent_csv = {\"1.Patent Number\":list(patent_number),\"2.Publication Date\":list(pub_date),\"3.Application Date\":list(app_date),\"4.Country\":list(country),\"5.Application Type\":list(app_type),\"6.Title\":list(patent_title),\"7.Abstract\":list(patent_abstract),\"8.Applicant\":list(applicant_name),\"9.Inventor\":list(inv_name),\"IPCs\":list(ipc_list)}\n",
        "\n",
        "type(patent_csv)\n",
        "\n",
        "patent_csv.pop('7.Abstract', None)\n",
        "\n",
        "print(len(patent_csv))\n",
        "pprint.pprint(patent_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "#使用Pandas匯出表格並存成CSV，也可以匯出給關聯式資料庫\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_dict(patent_csv,orient='index').T\n",
        "df.head(10)\n",
        "df.to_csv(\"patent_table.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Deal only claim texts\n",
        "from bs4 import BeautifulSoup\n",
        "import pprint\n",
        "path = \"patent_documents\"\n",
        "patent_docs_txt = {}\n",
        "for filename in os.listdir(path):\n",
        "  if filename.endswith(\".txt\"):\n",
        "    with open(os.path.join(path, filename), 'r') as f:\n",
        "      soup = BeautifulSoup(f.read(), 'xml')\n",
        "      tag = soup.find_all(\"claims\")\n",
        "      patent_docs_txt[filename] = tag[0].text.lower().replace('\\n',' ')\n",
        "\n",
        "pprint.pprint(patent_docs_txt, depth=1, width=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bljWYPMPpTSo",
        "outputId": "c391ecf0-20c2-4b69-cd25-5d6c09d5232a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.6.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\謝安\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\謝安\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'US08438662-20130514.txt': {'adf': 10, 'assembly': 34, 'behind': 11, 'claim': 39, 'comprises': 10, 'comprising': 15, 'configured': 10, 'cover': 17, 'different': 13, 'dimensions': 13, 'face': 47, 'filter': 9, 'first': 57, 'front': 30, 'interchangeable': 14, 'least': 19, 'lens': 160, 'lenses': 18, 'matching': 10, 'one': 9, 'opening': 24, 'plate': 19, 'portion': 11, 'properties': 10, 'protector': 31, 'releasably': 11, 'retainer': 10, 'second': 53, 'secured': 20, 'securing': 13, 'shade': 10, 'substantially': 21, 'transparent': 10, 'wearer': 14, 'wherein': 32},\n",
            " 'US08438663-20130514.txt': {'assembly': 27, 'back': 17, 'behind': 13, 'claim': 28, 'comprises': 11, 'comprising': 14, 'configured': 15, 'cover': 27, 'different': 10, 'extending': 8, 'face': 39, 'first': 27, 'frame': 14, 'front': 14, 'interchangeable': 9, 'least': 17, 'lens': 94, 'lenses': 15, 'magnifying': 10, 'one': 9, 'opening': 14, 'outer': 15, 'plate': 10, 'portion': 9, 'protector': 26, 'releasable': 7, 'releasably': 9, 'second': 36, 'secured': 11, 'spacer': 9, 'substantially': 11, 'wearer': 14, 'wherein': 22},\n",
            " 'US08438664-20130514.txt': {'accessible': 5, 'claim': 8, 'comprising': 3, 'connectable': 10, 'connected': 3, 'connection': 3, 'coupled': 3, 'deployed': 3, 'encircling': 3, 'end': 15, 'ends': 10, 'first': 17, 'forms': 3, 'front': 6, 'garment': 27, 'handle': 12, 'harness': 35, 'incorporating': 3, 'individual': 9, 'loop': 3, 'opposed': 24, 'pulling': 3, 'region': 10, 'rescue': 7, 'second': 17, 'strap': 15, 'thereof': 4, 'torso': 3, 'upper': 6, 'wearing': 3, 'when': 4, 'wherein': 17},\n",
            " 'US08438665-20130514.txt': {'accordance': 11, 'adapted': 15, 'attached': 8, 'barrel': 13, 'channel': 14, 'claim': 11, 'comprises': 9, 'configuration': 9, 'configured': 16, 'drawstring': 40, 'each': 24, 'extends': 10, 'face': 9, 'front': 10, 'garment': 26, 'hood': 46, 'locks': 11, 'lower': 9, 'margin': 17, 'neck': 26, 'one': 8, 'opening': 14, 'opposite': 11, 'portion': 60, 'portions': 26, 'rear': 10, 'retracted': 8, 'selectively': 11, 'shroud': 9, 'side': 38, 'top': 24, 'torso': 12, 'wearer': 22, 'wherein': 11},\n",
            " 'US08438666-20130514.txt': {'apparatus': 20, 'band': 92, 'channel': 40, 'claim': 17, 'configured': 13, 'coupled': 13, 'cover': 6, 'defining': 10, 'direction': 8, 'disposed': 9, 'ear': 27, 'end': 36, 'first': 95, 'including': 7, 'inner': 34, 'least': 8, 'member': 8, 'open': 6, 'opening': 8, 'opposite': 13, 'outer': 22, 'part': 7, 'passageway': 8, 'portion': 46, 'second': 80, 'shape': 8, 'side': 60, 'slidably': 7, 'slide': 6, 'surface': 48, 'surfaces': 26, 'wearer': 6, 'wherein': 16},\n",
            " 'US08438667-20130514.txt': {'apparel': 38, 'area': 17, 'article': 37, 'claim': 22, 'components': 7, 'comprising': 6, 'configuration': 6, 'element': 53, 'first': 49, 'foam': 6, 'fold': 44, 'form': 7, 'formed': 14, 'garment': 8, 'joined': 11, 'joint': 6, 'least': 12, 'located': 14, 'material': 16, 'one': 6, 'pad': 7, 'plurality': 8, 'polymer': 6, 'portion': 13, 'protective': 25, 'recited': 22, 'second': 50, 'substantially': 7, 'surface': 26, 'textile': 33, 'waistband': 11, 'wherein': 29},\n",
            " 'US08438668-20130514.txt': {'adjusting': 7, 'assembly': 6, 'attached': 21, 'branch': 11, 'chin': 19, 'claim': 19, 'claimed': 19, 'comprise': 7, 'comprises': 7, 'connected': 10, 'each': 7, 'end': 12, 'ends': 43, 'female': 10, 'first': 26, 'helmet': 18, 'lateral': 23, 'least': 19, 'left': 27, 'member': 8, 'members': 6, 'occipital': 30, 'one': 29, 'rear': 9, 'respectively': 9, 'right': 27, 'second': 29, 'section': 13, 'sections': 51, 'shaped': 5, 'shell': 18, 'sides': 8, 'stabilization': 8, 'strap': 30, 'wherein': 22},\n",
            " 'US08438669-20130514.txt': {'apparel': 32, 'article': 31, 'claim': 26, 'covering': 9, 'cushioning': 19, 'element': 20, 'elements': 9, 'fastening': 24, 'first': 22, 'foam': 9, 'includes': 6, 'including': 6, 'layer': 26, 'least': 6, 'located': 6, 'material': 41, 'pads': 14, 'part': 21, 'plate': 16, 'polymer': 18, 'portion': 13, 'recited': 26, 'second': 18, 'secured': 8, 'shoulder': 12, 'system': 24, 'torso': 14, 'wearer': 11, 'wherein': 26},\n",
            " 'US08438670-20130514.txt': {'according': 17, 'ajip': 8, 'ajop': 10, 'apm': 10, 'area': 36, 'arop': 10, 'bowl': 25, 'capable': 23, 'claim': 17, 'cycle': 29, 'delivering': 14, 'flow': 9, 'flush': 70, 'gravity': 21, 'greater': 29, 'jet': 12, 'least': 41, 'liters': 16, 'no': 21, 'one': 22, 'outlet': 20, 'port': 16, 'providing': 10, 'rate': 9, 'rim': 10, 'surface': 10, 'system': 37, 'than': 29, 'toilet': 48, 'two': 14, 'valve': 11, 'water': 13, 'wherein': 33},\n",
            " 'US08438671-20130514.txt': {'action': 14, 'actuator': 14, 'adjustment': 18, 'control': 16, 'cover': 44, 'cylinder': 16, 'each': 26, 'element': 16, 'elements': 16, 'extension': 19, 'front': 14, 'head': 17, 'housing': 23, 'hydraulic': 48, 'lid': 17, 'located': 16, 'longitudinal': 32, 'means': 14, 'pool': 76, 'position': 16, 'pressure': 18, 'seating': 28, 'short': 14, 'sidewall': 20, 'single': 14, 'structure': 33, 'system': 19, 'thrust': 14, 'top': 36, 'trough': 21, 'valve': 13}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "print(nltk.__version__)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def Patent_KW50(patent_text):\n",
        "  tokens = nltk.word_tokenize(patent_text)\n",
        "  fdist = nltk.FreqDist(tokens)\n",
        "  common50 = fdist.most_common(50)\n",
        "  common50_new1 = [(k,v) for k,v in common50 if not re.fullmatch(r\"^.$\", k)]\n",
        "  common50_new2 = [(k,v) for k,v in common50_new1 if re.fullmatch(r\"^[a-z]+$\", k, flags=re.IGNORECASE)]\n",
        "  stopword = list(nltk_stopwords)[:100]\n",
        "  common50_new3 = dict([(k,v) for k,v in common50_new2 if not k in stopword])\n",
        "  return common50_new3\n",
        "\n",
        "patent_docs_KW = {}\n",
        "for doc_name, txt in patent_docs_txt.items():\n",
        "  patent_docs_KW[doc_name] = Patent_KW50(txt)\n",
        "\n",
        "pprint.pprint(patent_docs_KW, width=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nOulhJ7g5mJS",
        "outputId": "1d5cfc00-cf48-4b85-e97f-88841993fdf9"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import islice\n",
        "\n",
        "plt.clf()\n",
        "\n",
        "for doc_name, KW_stat in patent_docs_KW.items():\n",
        "  common20 = dict(islice(KW_stat.items(), 20))\n",
        "  print(common20)\n",
        "  x = np.arange(20)\n",
        "  y = common20.values()\n",
        "  plt.title( doc_name, fontsize=8, color='g')\n",
        "  plt.xticks(x, common20.keys(),  rotation='vertical')\n",
        "  plt.bar(x, y, color='y')\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Patent_parser.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
